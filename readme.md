# Tree Benchmark

A compact benchmark suite for evaluating Binary Search Trees: AVL trees, Red-Black tress (referred to as `rb` in the code), and Treaps. The project supports a small demo run and a multi-size scaling benchmark that measures insert and lookup performance, throughput, and empirical false-positive rates, and saves results and plots.

## Features

* Demo run for a single, small-scale benchmark of 50,000 elements.
* Scaling benchmark across logarithmically spaced dataset sizes.
* Configurable experiment parameters, seeds, and plotting parameters.
* Saves JSON results and a PDF plot to a timestamped directory under the configured save path.

## Requirements

* Python 3.9 (recommended)
* matplotlib
* numpy
* PyYAML

While the project is expected to run with newer versions of Python, it was only tested with Python version 3.9.

## Recommended environment

Create and activate the recommended `conda` environment:

```bash
conda create -n tree_benchmark python=3.9 -y
conda activate tree_benchmark
```

## Install dependencies:

### Minimal setup 

This allows the usage of all the repository's functionality

```bash
pip install -r requirements.txt
```

## Running

Setup demo mode example in the config file:

```yaml
demo_run: true
scaling_benchmark_run: false
```

Setup scaling benchmark example in the config file:

```yaml
demo_run: false
scaling_benchmark_run: true
```

Run the main program:

```bash
python main.py
```

Note: If both `demo_run` and `scaling_benchmark_run` are true, the demo run takes priority and only the demo will execute.

## Unit Tests

Run unit tests with:

```bash
python -m unittest discover
```

## Dataset Generation

The dataset consists of pseudo-random strings generated by the utility function `generate_strings` in `utils/generate_data.py`.

### How it works

1. **Random strings** are built from uppercase letters, lowercase letters, and digits.

   ```python
   alphabet = string.ascii_letters + string.digits
   ```

   Each element is a string of fixed length (default 10 characters, 16 characters are used in the scaling dataset experiment) selected from this alphabet.

2. **Uniqueness guarantee**: A Python `set` is used to ensure no duplicate elements are included. The generator continues until the requested number of unique values has been created.

3. **Optional seed**: A random seed can be provided to make dataset generation reproducible across runs.

4. **Shuffling**: Once the set of values is complete, it is converted into a list and shuffled to randomize ordering.

5. **Returned length**: The actual length used for elements may be larger than the requested length to maintain a negligible collision probability. The function returns both the list of elements and the final length used.

Every benchmark run uses this mechanism to generate:

* The **dataset**: unique values inserted into the data structure.
* The **queries**: at most half sampled from the dataset (true positives), and at least half generated fresh with a different seed (false positives).

## Configuration

The configuration is read from `config/main.yaml`. The following YAML blocks show the configuration structure and the typed parameters expressed as YAML values. Special-case notes follow the YAML blocks where needed.

### Demo Configuration

For demo purposes, a sample configuration is placed in `config/main.yaml`. Feel free to edit the configuration file as needed.

### YAML Configuration Blocks 

#### Scaling benchmark parameters

```yaml
scaling_benchmark:
  min_items: 1000
  max_items: 200000
  num_steps: 8
  queries_ratio: 0.4
  fix_queries_ratio: true
  structures_to_test:
    - avl
    - treap
    - rb
  exclude_structures_above:
    - ["av;", 100000]
    - ["treap", 1000000]
  num_trials: 3
  seed: 42
  workload: random
  save_result_path: results
```

**Dataset:**
The dataset is the collection of strings generated for each benchmark size. Its size grows according to the logarithmic spacing defined by `min_items`, `max_items`, and `num_steps`. For example, with `min_items=1_000`, `max_items=200_000`, and `num_steps=8`, the system generates datasets of increasing sizes between 1,000 and 200,000.

**Queries:**
Queries are membership checks performed against the dataset. They consist of two halves:

* True positives: strings actually in the dataset.
* False positives: randomly generated strings not present in the dataset.

The total number of queries is controlled by `queries_ratio`. For example, `queries_ratio=0.4` means the system will issue 40% as many queries as the dataset size (or a fixed number if `fix_queries_ratio=true`). This ensures a balance between successful lookups and lookups that test the false-positive rate of probabilistic structures.

**Exclude structures above:**
Some data structures are impractical at very large sizes due to memory or runtime constraints. The `exclude_structures_above` setting allows disabling structures when the dataset size exceeds a threshold.

* Each entry is a pair `[structure_name, max_allowed_size]`.
* For example, `["linear", 100000]` means the linear search structure is only tested up to, and including, datasets of 100,000 items. Beyond that, it is skipped to avoid impractically slow runs.
* Plotting note: When a structure is excluded for larger dataset sizes, its performance curve is linearly extrapolated from its last valid measurement to ensure all structures remain comparable in the plots. Extrapolated segments are dashed.

**num_trials:**
The number of times each benchmark is repeated for a given dataset size. Repeating the benchmark smooths out noise from system fluctuations (e.g., CPU load, memory state). The median value across trials is reported for each metric (insert time, lookup time, operations per second).

**workload:**
The workload defines how the dataset is inserted into the data structure and how queries are issued. Different workloads simulate varying access patterns and insertion orders, which can stress different aspects of the data structures.

Options:
- `random`  
  - Insertion order: Dataset is inserted in a random order.  
  - Queries: Standard queries drawn from the full dataset.  
  - Purpose: Represents a typical usage scenario with non-ordered insertions. Balanced trees perform near their average-case complexity.  

- `ascending`  
  - Insertion order: Dataset is sorted in ascending order before insertion.  
  - Queries: Standard queries drawn from the full dataset.  
  - Purpose: Simulates sequential inserts. Can create unbalanced trees in plain BSTs, while AVL and RB trees perform rotations to maintain balance.  

- `descending`  
  - Insertion order: Dataset is sorted in descending order before insertion.  
  - Queries: Standard queries drawn from the full dataset.  
  - Purpose: Symmetric to ascending inserts. Stresses rotations in balanced trees and is a worst-case scenario for naive BSTs.  

- `hotspot`  
  - Insertion order: Dataset is inserted in its original order.  
  - Queries: Skewed, 80% of queries target the first 10% of the dataset (the “hot” portion) and 20% drawn uniformly from the full dataset.  
  - Purpose: Simulates scenarios with frequently accessed “hot” data. 

**save_result_path:**
Directory where benchmark outputs are stored. When set, the program saves results into a timestamped subdirectory under this path. For example: `results/20251022_143210/`. Inside this directory:

* A JSON file containing raw benchmark results.
* A PDF plot showing performance trends across dataset sizes.

If left unset (`null`), results are not saved to disk; only plots are shown interactively.

#### Print / Plot Configuration

```yaml
print_config:
  plot_avg_per_op: true
  print_avg_per_op: true
  plot_throughput: false
  print_throughput: false
  plot_rotations: true
  print_rotations: true
  plot_heights: true
  print_heights: true
  plot_validation: true
  print_validation: true
  plot_balance: true
  print_balance: true
```

This configuration is a dictionary that controls which metrics are printed to the console and which are plotted. Only the recognized keys are considered; unknown keys are ignored. The recognized options are:

* `print_avg_per_op`
* `print_throughput`
* `print_rotations`
* `print_heights`
* `print_validation`
* `print_balance`
* `plot_avg_per_op`
* `plot_throughput`
* `plot_rotations`
* `plot_heights`
* `plot_validation`
* `plot_balance`

**Behavior:**

1. **Average time per operation (`avg_per_op`)**

  * If `print_avg_per_op` is `true`, the benchmark prints the average time of insert, lookup, and delete operations per query rather than cumulative time.
  * If `plot_avg_per_op` is `true`, the plot shows average time per operation instead of cumulative operation time.
  * This helps normalize performance across datasets of different sizes and makes comparisons more meaningful.

2. **Throughput (`throughput`)**

  * If `print_throughput` is `true`, prints the number of insert, lookup, and delete operations per second.
  * If `plot_throughput` is `true`, throughput curves are included in the PDF plots.

3. **Number of rotations (`rotations`)**

  * If `print_rotations` is `true`, prints the total number of rotations performed after insertion and deletion.
  * If `plot_rotations` is `true`, plots the total number rotations performed after insertion and deletion.

4. **Heights (`heights`)**

  * If `print_heights` is `true`, prints the height of the trees after insertion and deletion.
  * If `plot_heights` is `true`, plots the height of the trees after insertion and deletion.

5. **Validate trees (`validation`)**

  * If `print_validation` is `true`, prints whether the trees conform to their requirements.
  * If `plot_validation` is `true`, plots a heatmap showing whether the trees conform to their requirements across different dataset sizes.

4. **Tree Balance (`balance`)**

  * If `print_balance` is `true`, prints the balance of the trees after insertion and deletion alongside the depth of the tree.
  * If `plot_balance` is `true`, plots the balance of the trees after insertion and deletion alongside the depth of the tree.

Notes:

* All options default to `true` if not explicitly provided.
* Only recognized keys are applied; any unknown keys in the dictionary are ignored.
* In the saved `json` results, the values are not averaged out even if the `avg_per_op` is set to `true`.
* The saved results can be re-plotted using `python plot_saved_result.py RESULT_JSON_PATH [--config_path CONFIG_PATH] [--save_path SAVE_PATH]`. Please check the script for more description.

#### Defaults values

```yaml
defaults:
  min_items: 1000
  max_items: 200000
  num_steps: 8
  queries_ratio: 0.4
  fix_queries_ratio: true
  workload: random
  num_trials: 3
  seed: 42
```

## Docstring

This project uses *NumPy-style docstrings*. Comments and docstrings in the codebase were generated with assistance from ChatGPT.

## Acknowledgements

AVL implementation in this project closely follows the description from: 
```bibtex
@misc{Kartik_avl_insert_impl,
  title   = {Insertion in an AVL tree},
  url     = {https://www.geeksforgeeks.org/dsa/insertion-in-an-avl-tree/},
  journal = {GeeksforGeeks},
  author  = {Kartik},
  year    = {2025},
  month   = {Jul}
} 

@misc{Kartik_avl_deletion_impl,
  title   = {Deletion in an AVL tree},
  url     = {https://www.geeksforgeeks.org/dsa/deletion-in-an-avl-tree/},
  journal = {GeeksforGeeks},
  author  = {Kartik},
  year    = {2025},
  month   = {Jun}
} 
```

Red-black implementation follows the design outlined in:
```bibtex
@book{clrs,
  author    = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
  title     = {Introduction to Algorithms, Third Edition},
  year      = {2009},
  isbn      = {0262033844},
  publisher = {The MIT Press},
  edition   = {3rd}
}
```

Treap implementation follows the design outlined in:
```bibtex
@misc{Kartik_treap_imp,
  title     = {Implementation of search, insert and delete in Treap}, 
  url       = {https://www.geeksforgeeks.org/dsa/implementation-of-search-insert-and-delete-in-treap/}, 
  journal   = {GeeksforGeeks}, 
  publisher = {GeeksforGeeks}, 
  author    = {Kartik}, 
  year      = {2025}, 
  month     = {Jul}
} 
```
